# -*- coding: utf-8 -*-
"""Untitled68.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmPMIFjm83UiG4uMi41H8g7OvSh3S4SO

**Importing the Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import csv

"""**Importing the dataset**"""

dataset=pd.read_csv('twitter_training.csv')

"""**Removing the Quotes**"""

dataset.iloc[:,3]=dataset.iloc[:,3].replace('"','')

"""**Decoding the Dataset**"""

dataset.head()

dataset.columns

dataset.describe()

dataset.info()

"""**Checking for missing values**"""

dataset.isnull().sum()

"""**Handling Missing Values**"""

dataset=dataset.dropna(subset=['Tweet_content'])

"""**Exploring the Sentiment Column**"""

dataset['sentiment'].unique()

"""Insights : There are 4 different sentiments predicted based on the tweet contents

**Plotting the Sentiment count**
"""

sentiment_count=dataset.groupby('sentiment').count()
sentiment_labels=dataset['sentiment'].unique()

plt.pie(sentiment_count['Tweet_ID'],labels=sentiment_labels,autopct='%1.1f%%')

"""Insights :

1) Negative tweets are highest in count

2)Irrelevant tweet are least in count

3) Neutral and Positive lie in between

**Exploring the entity Column**
"""

dataset['entity'].unique()

"""**Plotting the entity column count**"""

entity_count=dataset.groupby('entity').count()
entity_labels=dataset['entity'].unique()

plt.figure(figsize=(10, 10))
plt.pie(entity_count['Tweet_ID'],labels=entity_labels)

"""Insights : It seems like all the entity have almost equal weight in the dataset

**Removing the Unneccesary Feature**

The tweet ID plays no role here so it is better having it removed
"""

dataset=dataset.drop('Tweet_ID',axis=1)

"""**Applying NLP**

**Cleaning the Dataset**

Cleaning the Training set
"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, 73996):
  review = re.sub('[^a-zA-Z]', ' ', dataset.iloc[i,2])
  review = review.lower()
  review = review.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
  review = ' '.join(review)
  corpus.append(review)

print(corpus)

"""**Creating Bag of Words Model**"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 1500)
X= cv.fit_transform(corpus).toarray()
Y=dataset.iloc[:,1]

"""**Encoding the target feature**"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
Y=le.fit_transform(Y)

"""**Splitting into training and test set**"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)

"""**Training the Naive Bayes model on training set**"""

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=300,criterion='entropy',random_state=0)
classifier.fit(X_train,Y_train)

Y_pred = classifier.predict(X_test)
print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))

print(Y_pred)

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(Y_test, Y_pred)
print(cm)
accuracy_score(Y_test, Y_pred)